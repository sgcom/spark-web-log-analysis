{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring a public web log data set using PySpark\n",
    "\n",
    "This notebook uses Apache Spark to explore a publicly available web log data set. \n",
    "\n",
    "This is the first part, which does ETL - reads the raw log file, parses each records, and loads it into columnar Parquet format. Then the parquet data store will be available for further analysis. \n",
    "\n",
    "The dataset is available from [The Internet Traffic Archive](http://ita.ee.lbl.gov/index.html).\n",
    "\n",
    "#### Dataset source: http://ita.ee.lbl.gov/html/contrib/Sask-HTTP.html\n",
    "\n",
    "#### Dataset description:\n",
    "\n",
    "This trace contains seven months’ worth of all HTTP requests to the University of Saskatchewan's WWW server. The University of Saskatchewan is located in Saskatoon, Saskatchewan, Canada.\n",
    "\n",
    "The log contains over **_2,400,000_** lines from **_June to December 1995_** - the early days of the World Wide Web."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Parse the log file and load its contents into parquet format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the Apache Spark context & config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Initialize\n",
    "#\n",
    "import findspark\n",
    "import os\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'1.5.0',\n",
       " [(u'spark.driver.memory', u'6g'),\n",
       "  (u'spark.rdd.compress', u'True'),\n",
       "  (u'spark.master', u'local[8]'),\n",
       "  (u'spark.serializer.objectStreamReset', u'100'),\n",
       "  (u'spark.submit.deployMode', u'client'),\n",
       "  (u'spark.app.name', u'pyspark-shell')])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the Spark context & config\n",
    "#\n",
    "sc.version, sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import time, datetime\n",
    "import os\n",
    "import operator as op\n",
    "from pyspark.sql import Row, SQLContext\n",
    "from pyspark import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data file location settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/UofS_access_log\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = './data'\n",
    "LOG_FILE_NAME = 'UofS_access_log'\n",
    "\n",
    "logFileName = os.path.join(DATA_DIR, LOG_FILE_NAME)\n",
    "print logFileName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2408625 ./data/UofS_access_log\r\n"
     ]
    }
   ],
   "source": [
    "# Using the Unix wc command to get the number of lines in the log file\n",
    "#\n",
    "!wc -l ./data/UofS_access_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular expression to use for parsing a log line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_log_line = re.compile(r'^(\\S+)\\s(\\S+)\\s(\\S+)\\s\\[(\\d*)/([A-Za-z]+)/(\\d+):(\\d*):(\\d*):(\\d*) .*\\]\\s\"([A-Z]+) +(\\S+)[^\"]*\" (\\d+) (\\S+)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse log  line function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "month2number_map = {\n",
    "    'Jan': 1, 'Feb': 2, 'Mar':3, 'Apr':4, 'May':5, 'Jun':6, 'Jul':7,\n",
    "    'Aug':8,  'Sep': 9, 'Oct':10, 'Nov': 11, 'Dec': 12 \n",
    "}\n",
    "\n",
    "def parse_log_line(line):\n",
    "    m = p_log_line.match(line)\n",
    "    if m:\n",
    "        dtm = datetime.datetime(int(m.group(6)),\n",
    "                             month2number_map[m.group(5)],\n",
    "                             int(m.group(4)),\n",
    "                             int(m.group(7)),\n",
    "                             int(m.group(8)),\n",
    "                             int(m.group(9)))\n",
    "        date = datetime.date(dtm.year, dtm.month, dtm.day)\n",
    "        \n",
    "        epo_seconds = (dtm - datetime.datetime(1970,1,1)).total_seconds()\n",
    "        respsize = 0\n",
    "        if m.group(13) != '-':\n",
    "            respsize = int(m.group(13))\n",
    "        return( Row(\n",
    "            remote_host = m.group(1), # host name or IP, where the request comes from\n",
    "            user_id     = m.group(3), \n",
    "            date_time   = dtm,\n",
    "            date        = datetime.date(dtm.year, dtm.month, dtm.day),\n",
    "            year        = dtm.year,\n",
    "            month       = dtm.month,\n",
    "            day         = dtm.day,\n",
    "            hours       = dtm.hour,\n",
    "            minutes     = dtm.minute,\n",
    "            quarter_hour= dtm.hour + int(dtm.minute)/15 * 0.25,\n",
    "            week_day    = dtm.weekday() + 1,\n",
    "            epo_seconds = epo_seconds,\n",
    "            http_verb   = m.group(10), \n",
    "            url         = m.group(11), \n",
    "            http_status = int(m.group(12)), \n",
    "            resp_size   = respsize, \n",
    "        ), 1)             ### line parsed successfully \n",
    "    return (line, 0)      ### not parsed successfully     \n",
    "\n",
    "#\n",
    "# TODO: Now that Spark 1.5 has a nice set of date/time functions, this can probably be modified to \n",
    "#       not load redundently year, month, ... \n",
    "##################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for lines that failed to parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Uncomment this if need to check for lines that failed to parse.\n",
    "## Some 45 lines won't not parse successfully. Looking at them, \n",
    "## they have rally strange format. Let's say we can do without them.\n",
    "#\n",
    "# not_parsed_rdd = (sc\n",
    "#                    .textFile(logFileName)\n",
    "#                    .map(parse_log_line)\n",
    "#                    .filter(lambda s: s[1] == 0)\n",
    "#                    .map(lambda s: s[0]) )\n",
    "\n",
    "# print \"not parsed: \", not_parsed_rdd.count(), ' lines'\n",
    "# for x in not_parsed_rdd.take(10):\n",
    "#     print x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the successfully parsed lines into a RDD.  \n",
    "\n",
    "Then check out the number of lines and what the result looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.18 ms, sys: 853 µs, total: 9.04 ms\n",
      "Wall time: 34.9 s\n",
      "parsed successfully:  2408580  lines\n",
      "Row(date=datetime.date(1995, 6, 1), date_time=datetime.datetime(1995, 6, 1, 0, 0, 59), day=1, epo_seconds=801964859.0, hours=0, http_status=200, http_verb=u'GET', minutes=0, month=6, quarter_hour=0.0, remote_host=u'202.32.92.47', resp_size=271, url=u'/~scottp/publish.html', user_id=u'-', week_day=4, year=1995)\n"
     ]
    }
   ],
   "source": [
    "parsed_rdd = (sc\n",
    "               .textFile(logFileName)\n",
    "               .map(parse_log_line)\n",
    "               .filter(lambda s: s[1] == 1)\n",
    "               .map(lambda s: s[0])\n",
    "               .persist(StorageLevel.MEMORY_AND_DISK))\n",
    "%time count_parsed = parsed_rdd.count()\n",
    "print \"parsed successfully: \", count_parsed, ' lines'\n",
    "print parsed_rdd.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform to a \"hits by day\" RDD; then check out the date range and a few counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of days:  214 \n",
      "from: 1995-06-01 Thu \n",
      "  to: 1995-12-31 Sun\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('1995-06-01 Thu', 5589),\n",
       " ('1995-06-02 Fri', 7219),\n",
       " ('1995-06-03 Sat', 4025),\n",
       " ('1995-06-04 Sun', 3638),\n",
       " ('1995-06-05 Mon', 6978),\n",
       " ('1995-06-06 Tue', 8930),\n",
       " ('1995-06-07 Wed', 7919),\n",
       " ('1995-06-08 Thu', 8664),\n",
       " ('1995-06-09 Fri', 7500),\n",
       " ('1995-06-10 Sat', 4487)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hits_by_day_rdd = ( parsed_rdd\n",
    "    .map(lambda x: ('%04d-%02d-%02d %s' % \n",
    "                    ( x.date_time.year,\n",
    "                      x.date_time.month,\n",
    "                      x.date_time.day, \n",
    "                      month2number_map[x.week_day] ),\n",
    "                    1) )\n",
    "    .reduceByKey(op.add)\n",
    "    .sortByKey()\n",
    "    .cache() )\n",
    "\n",
    "count = hits_by_day_rdd.count()\n",
    "first = hits_by_day_rdd.first()[0]\n",
    "last = hits_by_day_rdd.sortByKey(False).first()[0]\n",
    "\n",
    "print 'Total number of days: ', count, '\\nfrom:', first, '\\n  to:', last\n",
    "\n",
    "hits_by_day_rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we will save the log data in parquet format for further analysis. \n",
    "#### This is to be done only once. It will give error if the \"file\" is already there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: date (nullable = true)\n",
      " |-- date_time: timestamp (nullable = true)\n",
      " |-- day: long (nullable = true)\n",
      " |-- epo_seconds: double (nullable = true)\n",
      " |-- hours: long (nullable = true)\n",
      " |-- http_status: long (nullable = true)\n",
      " |-- http_verb: string (nullable = true)\n",
      " |-- minutes: long (nullable = true)\n",
      " |-- month: long (nullable = true)\n",
      " |-- quarter_hour: double (nullable = true)\n",
      " |-- remote_host: string (nullable = true)\n",
      " |-- resp_size: long (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- week_day: long (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Save to parquet format\n",
    "## Do this only once; it will give error if the \"file\" is already there.\n",
    "#\n",
    "sqlContext = SQLContext(sc)\n",
    "weblogDf   = sqlContext.createDataFrame(parsed_rdd)\n",
    "#\n",
    "weblogDf.write.parquet(\"data/UofS_access_log.parquet\")\n",
    "weblogDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Load done.  Will read the data and explore it further in another notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
